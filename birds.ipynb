{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cerDfFtIWrXn",
    "outputId": "c560f16b-ed6e-40a0-bf44-420214f2bdc0"
   },
   "outputs": [],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpZcPUCtXZ2n",
    "outputId": "814c2436-1466-4823-a704-8422e1fff067",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install split_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZVd-g3EB9587"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy.signal as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from librosa import core, onset, feature, display\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import copy\n",
    "import os\n",
    "import pathlib\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import splitfolders\n",
    "import keras, keras.layers\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD \n",
    "import tensorflow as tf \n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications import EfficientNetB3 as efn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aCmHE-dViu0"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"birdsong_metadata.csv\")\n",
    "#dataset.head()\n",
    "x = []\n",
    "samplerate = []\n",
    "spectrum = []\n",
    "spectrogram = []\n",
    "\n",
    "\n",
    "#Delete the useless collumns of y \n",
    "y = dataset.iloc[:, 3]\n",
    "y = np.array(y)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUmiJl-LfjlO"
   },
   "source": [
    "# Convert the audios to corresponding Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uk2VcYm8m3zl",
    "outputId": "7f1f3fa8-27dc-4ada-86ed-2d7fe76f43a8"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "\n",
    "pathlib.Path('./Birdsongs_Spectrograms').mkdir(parents=True, exist_ok=True)\n",
    "birds_names = dataset.iloc[:, 3].unique()\n",
    "for birds_name in birds_names:\n",
    "  pathlib.Path(f'./Birdsongs_Spectrograms/{birds_name}').mkdir(parents=True, exist_ok=True)  \n",
    "  for i in range(len(dataset)):\n",
    "    if os.path.exists(f'./Birdsongs_Spectrograms/{dataset.iloc[i, 3]}'):\n",
    "      if len(os.listdir(f'./Birdsongs_Spectrograms/{dataset.iloc[i, 3]}')) < 3: \n",
    "        x.append(sf.read('songs/xc' + str(dataset['file_id'][i]) + '.flac'))\n",
    "        samplerate.append(x[i][1])\n",
    "        plt.specgram(x[i][0], NFFT = 2048, Fs=samplerate[i], Fc=0, noverlap=210, sides='default', mode='default', scale='dB');\n",
    "        plt.axis('off');\n",
    "        print(dataset.iloc[i, 3] + ' - ' + str(dataset.iloc[i, 0]))\n",
    "        plt.savefig('./Birdsongs_Spectrograms/' + dataset.iloc[i, 3] +'/' + str(dataset['file_id'][i]) + '.png')\n",
    "        plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train and test folders and create train and test batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "9EqItBwzb5Yp",
    "outputId": "d38a52ac-47eb-46b2-8f7e-62153870c0b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 264 files [00:00, 596.30 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 88 classes.\n",
      "Found 88 images belonging to 88 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitfolders.ratio('./Birdsongs_Spectrograms/', output = './data/', seed = 1337, ratio=(.8, .2))\n",
    "\n",
    "image_size = (64, 64) \n",
    "path = \"./data/\"\n",
    "batch_size = 16\n",
    "\n",
    "train_data_generator = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                                          shear_range = 0.2, zoom_range = 0.1, fill_mode = 'nearest')\n",
    "train_batches = train_data_generator.flow_from_directory(path + 'train', target_size = image_size, \n",
    "                                                         class_mode = 'categorical', shuffle = False, \n",
    "                                                         batch_size = batch_size)\n",
    "\n",
    "test_data_generator = ImageDataGenerator()\n",
    "test_batches = test_data_generator.flow_from_directory(path + 'val', \n",
    "                                                 target_size = image_size, \n",
    "                                                 class_mode = 'categorical', shuffle = False, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model with a poor accuracy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Qn-l7FzMeHGB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 31, 31, 32)        896       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_21 (Averag (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_22 (Averag (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_23 (Averag (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               57700     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 88)                8888      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 88)                0         \n",
      "=================================================================\n",
      "Total params: 122,908\n",
      "Trainable params: 122,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "11/50 [=====>........................] - ETA: 3s - loss: 23.1347 - accuracy: 0.0000e+00WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 7500 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 11 batches). You may need to use the repeat() function when building your dataset.\n",
      "50/50 [==============================] - 3s 37ms/step - loss: 15.4641 - accuracy: 0.0000e+00 - val_loss: 4.4821 - val_accuracy: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/50 [==>...........................] - ETA: 2sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "50/50 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "input_shape=(64, 64, 3)\n",
    "model.add(Conv2D(32, (3, 3), strides=(2, 2), input_shape=input_shape))\n",
    "model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(88))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "decay_rate = learning_rate / epochs\n",
    "optimizer = SGD(lr = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
    "model.compile(optimizer = optimizer, loss =\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_batches, steps_per_epoch = 50, epochs = 150, validation_data = test_batches, \n",
    "                    validation_steps = int(88/batch_size))\n",
    "model.evaluate_generator(generator = test_batches, steps = 50)\n",
    "\n",
    "test_batches.reset()\n",
    "prediction = model.predict_generator(test_batches, steps = 50, verbose = 1)\n",
    "\n",
    "predicted_class_indices=np.argmax(prediction, axis=1)\n",
    "labels = (train_batches.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "predictions = predictions[:200]\n",
    "filenames=test_batches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'birdsongs_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-b7a044cded77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[0mbird_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[0mto_append\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf'{ bird_name}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'birdsongs_features.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_append\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'birdsongs_features.csv'"
     ]
    }
   ],
   "source": [
    "with open('birdsongs_features.csv', 'w', newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Spectral Centroid', 'Spectral Rolloff',  'Spectral Bandwidth', 'Zero-Crossing Rate', \n",
    "                    'Tone', 'Chroma SFTF', 'Mel-Frequency Cepstral Coefficients'])\n",
    "#mfcc = []\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "  X, s_rate = sf.read('songs/xc' + str(dataset['file_id'][i]) + '.flac')\n",
    "  mf = librosa.feature.mfcc(y = X, sr = s_rate)\n",
    "  cf = librosa.feature.chroma_stft(X, s_rate)\n",
    "  sc = librosa.feature.spectral_centroid(X, s_rate)\n",
    "  sb = librosa.feature.spectral_bandwidth(X, s_rate)\n",
    "  sroll = librosa.feature.spectral_rolloff(X, s_rate)\n",
    "  zcr = librosa.feature.zero_crossing_rate(X)\n",
    "    \n",
    "  t = np.mean(librosa.feature.tonnetz(\n",
    "             y=librosa.effects.harmonic(X),\n",
    "             sr=s_rate).T,axis=0)\n",
    "  #to_append = dataset.iloc[i, 3]  \n",
    "  to_append = f'{np.mean(sc)} {np.mean(sroll)} {np.mean(sb)} {np.mean(zcr)} {t} {np.mean(cf)}'\n",
    "  for e in mf:\n",
    "    to_append += f' {np.mean(e)}'\n",
    "     \n",
    "  bird_name = dataset.iloc[i, 3]\n",
    "  to_append += f'{ bird_name}'    \n",
    "  with open('birdsongs_features.csv', 'a', newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([to_append])\n",
    "\n",
    "  #print(dataset.iloc[i, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the new dataset and perform the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = pd.read_csv('birdsongs_features.csv', error_bad_lines = False)\n",
    "features_dataset.head()\n",
    "x = features_dataset.iloc[:, 1:]\n",
    "y= features_dataset.iloc[:, 0]\n",
    "\n",
    "#print(x)\n",
    "#print(y)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "for i in range(len(features_dataset)):\n",
    "  x.iloc[i:, 4] = np.asfarray(x.iloc[i:, 4])\n",
    "#np.array(x)\n",
    "#x = x.strip()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(np.asfarray(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(88, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "classifier = model.fit(X_train, y_train, epochs=400, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
